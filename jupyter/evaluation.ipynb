{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:32:21.463352Z",
     "start_time": "2024-03-28T13:32:14.661690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUP (First 1000)\n",
      "\t166/1000 correct predictions\n",
      "\tAccuracy: 0.166\n",
      "\tEd: 2.592\n",
      "\tRed: 0.9276436507936507\n",
      "\tGleu: 65.49282583663754\n",
      "\tMeteor: 80.01465187784005\n",
      "HebCup (First 1000)\n",
      "\t121/1000 correct predictions\n",
      "\tAccuracy: 0.121\n",
      "\tEd: 2.974\n",
      "\tRed: 1.1776805194805195\n",
      "\tGleu: 61.455580947970724\n",
      "\tMeteor: 80.31157918256051\n",
      "LLM (First 1000)\n",
      "\t177/1000 correct predictions\n",
      "\t255/1000 correct predictions\n",
      "\tAccuracy: 0.177\n",
      "\tRecall@5: 0.255\n",
      "Top1:\n",
      "\tEd: 8.298\n",
      "\tRed: 4.435\n",
      "\tGleu: 50.430\n",
      "\tMeteor: 71.512\n",
      "Theoretical best in top5:\n",
      "\tEd: 4.518\n",
      "\tRed: 2.225\n",
      "\tGleu: 62.952\n",
      "\tMeteor: 80.964\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "import eval\n",
    "\n",
    "\n",
    "def evaluate(result_array):\n",
    "    stats_dict = {}\n",
    "    for metric in metrics:\n",
    "        avg = statistics.mean(map(lambda line: line[metric], result_array))\n",
    "        stats_dict[metric] = avg\n",
    "    nr_correct = len(list(filter(lambda line: line['ed'] == 0, result_array)))\n",
    "    stats_dict['accuracy'] = nr_correct / len(result_array)\n",
    "    print(f'\\t{nr_correct}/{len(result_array)} correct predictions')\n",
    "\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "metrics = ['ed', 'red', 'gleu', 'meteor']\n",
    "metrics1 = ['accuracy'] + metrics\n",
    "\n",
    "#Baseline\n",
    "\n",
    "## CUP (First 1000)\n",
    "print('CUP (First 1000)')\n",
    "cup_result = eval.evaluate('../result/baseline/CUP_first1000.jsonl', 'CUP')[:1000]\n",
    "cup_d = evaluate(cup_result)\n",
    "for metric_ in metrics1:\n",
    "    print(f'\\t{metric_.capitalize()}: {cup_d[metric_]}')\n",
    "\n",
    "## HebCup (First 1000)\n",
    "print('HebCup (First 1000)')\n",
    "hebcup_result = eval.evaluate('../result/baseline/HebCup_first1000.jsonl', 'HebCup')[:1000]\n",
    "hebcup_d = evaluate(hebcup_result)\n",
    "for metric_ in metrics1:\n",
    "    print(f'\\t{metric_.capitalize()}: {hebcup_d[metric_]}')\n",
    "\n",
    "# LLM\n",
    "result = eval.evaluate('../result/candidates/candidates-20240328_154708.jsonl')\n",
    "\n",
    "count = len(result)\n",
    "\n",
    "print(f'LLM (First 1000)')\n",
    "\n",
    "simple_top1 = list(map(lambda x: x[0], result))\n",
    "d1 = evaluate(simple_top1)\n",
    "\n",
    "theoretical_best = list(map(lambda candidates: sorted(candidates, key=lambda cand: cand['gleu'], reverse=True)[0], result))\n",
    "d2 = evaluate(theoretical_best)\n",
    "\n",
    "print(f'\\tAccuracy: {d1[\"accuracy\"]:}')\n",
    "print(f'\\tRecall@5: {d2[\"accuracy\"]:}')\n",
    "\n",
    "print('Top1:')\n",
    "for metric_ in metrics:\n",
    "    print(f'\\t{metric_.capitalize()}: {d1[metric_]:.3f}')\n",
    "\n",
    "print('Theoretical best in top5:')\n",
    "for metric_ in metrics:\n",
    "    print(f'\\t{metric_.capitalize()}: {d2[metric_]:.3f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
