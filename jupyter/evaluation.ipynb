{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-28T12:38:18.101080Z",
     "start_time": "2024-03-28T12:38:07.979167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUP (First 1000)\n",
      "\t163/1000 correct predictions\n",
      "\tAccuracy: 0.163\n",
      "\tEd: 2.845\n",
      "\tRed: 0.9499214285714286\n",
      "\tGleu: 68.11386381064486\n",
      "\tMeteor: 83.7002437317202\n",
      "HebCup (First 1000)\n",
      "\t155/1000 correct predictions\n",
      "\tAccuracy: 0.155\n",
      "\tEd: 2.95\n",
      "\tRed: 0.9766881313131313\n",
      "\tGleu: 66.7704118548645\n",
      "\tMeteor: 84.84403325038214\n",
      "LLM (First 1000)\n",
      "\t158/1000 correct predictions\n",
      "\t226/1000 correct predictions\n",
      "\tAccuracy: 0.158\n",
      "\tRecall@5: 0.226\n",
      "Top1:\n",
      "\tEd: 9.432\n",
      "\tRed: 4.585\n",
      "\tGleu: 52.644\n",
      "\tMeteor: 74.794\n",
      "Theoretical best in top5:\n",
      "\tEd: 5.497\n",
      "\tRed: 2.560\n",
      "\tGleu: 63.788\n",
      "\tMeteor: 83.964\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "import eval\n",
    "\n",
    "\n",
    "def evaluate(result_array):\n",
    "    stats_dict = {}\n",
    "    for metric in metrics:\n",
    "        avg = statistics.mean(map(lambda line: line[metric], result_array))\n",
    "        stats_dict[metric] = avg\n",
    "    nr_correct = len(list(filter(lambda line: line['ed'] == 0, result_array)))\n",
    "    stats_dict['accuracy'] = nr_correct / len(result_array)\n",
    "    print(f'\\t{nr_correct}/{len(result_array)} correct predictions')\n",
    "\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "metrics = ['ed', 'red', 'gleu', 'meteor']\n",
    "metrics1 = ['accuracy'] + metrics\n",
    "\n",
    "#Baseline\n",
    "\n",
    "## CUP (First 1000)\n",
    "print('CUP (First 1000)')\n",
    "cup_result = eval.evaluate('../result/baseline/CUP_first1000.jsonl', 'CUP')[:1000]\n",
    "cup_d = evaluate(cup_result)\n",
    "for metric_ in metrics1:\n",
    "    print(f'\\t{metric_.capitalize()}: {cup_d[metric_]}')\n",
    "\n",
    "## HebCup (First 1000)\n",
    "print('HebCup (First 1000)')\n",
    "hebcup_result = eval.evaluate('../result/baseline/HebCup_first1000.jsonl', 'HebCup')[:1000]\n",
    "hebcup_d = evaluate(hebcup_result)\n",
    "for metric_ in metrics1:\n",
    "    print(f'\\t{metric_.capitalize()}: {hebcup_d[metric_]}')\n",
    "\n",
    "# LLM\n",
    "result = eval.evaluate('../result/candidates/candidates-20240328_154708.jsonl')\n",
    "\n",
    "count = len(result)\n",
    "\n",
    "print(f'LLM (First 1000)')\n",
    "\n",
    "simple_top1 = list(map(lambda x: x[0], result))\n",
    "d1 = evaluate(simple_top1)\n",
    "\n",
    "theoretical_best = list(map(lambda candidates: sorted(candidates, key=lambda cand: cand['meteor'], reverse=True)[0], result))\n",
    "d2 = evaluate(theoretical_best)\n",
    "\n",
    "print(f'\\tAccuracy: {d1[\"accuracy\"]:}')\n",
    "print(f'\\tRecall@5: {d2[\"accuracy\"]:}')\n",
    "\n",
    "print('Top1:')\n",
    "for metric_ in metrics:\n",
    "    print(f'\\t{metric_.capitalize()}: {d1[metric_]:.3f}')\n",
    "\n",
    "print('Theoretical best in top5:')\n",
    "for metric_ in metrics:\n",
    "    print(f'\\t{metric_.capitalize()}: {d2[metric_]:.3f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
