{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-27T08:59:00.566592Z",
     "start_time": "2024-04-27T08:58:55.817239Z"
    }
   },
   "source": [
    "import statistics\n",
    "\n",
    "import pandas\n",
    "\n",
    "import eval\n",
    "from sorter import *\n",
    "\n",
    "\n",
    "def evaluate(result_array):\n",
    "    stats_dict = {}\n",
    "    for metric in metrics:\n",
    "        ll = list(map(lambda line: line[metric], result_array))\n",
    "        avg = statistics.mean(ll)\n",
    "        stats_dict[metric] = f'{avg:.3f}'\n",
    "    nr_correct = len(list(filter(lambda line: line['ed'] == 0, result_array)))\n",
    "    stats_dict['accuracy'] = f'{nr_correct / len(result_array):.3f}'\n",
    "    stats_dict['correct/total'] = f'\\t{nr_correct}/{len(result_array)}'\n",
    "\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "def print_all_metrics(d):\n",
    "    for metric in metrics1:\n",
    "        print(f'\\t{metric.capitalize()}: {d[metric]:.3f}')\n",
    "\n",
    "\n",
    "class ResultTable:\n",
    "    def __init__(self):\n",
    "        self.rows = []\n",
    "\n",
    "    def add(self, title: str, d: dict):\n",
    "        row = {'Method': title}\n",
    "        row.update({metric.capitalize(): d[metric] for metric in metrics1})\n",
    "        self.rows.append(row)\n",
    "\n",
    "    def to_markdown(self):\n",
    "        df = pandas.DataFrame(self.rows)\n",
    "\n",
    "        # Convert DataFrame to Markdown table without index\n",
    "        return df.to_markdown(index=False)\n",
    "\n",
    "\n",
    "metrics = ['ed', 'red', 'gleu', 'meteor', 'rouge-recall', 'rouge-f1']\n",
    "metrics1 = ['correct/total', 'accuracy'] + metrics\n",
    "rt = ResultTable()\n",
    "\n",
    "#Baseline\n",
    "\n",
    "# ## CUP (First 1000)\n",
    "# cup_result = eval.evaluate('../result/baseline/CUP.jsonl', 'CUP')\n",
    "# cup_d = evaluate(cup_result)\n",
    "# rt.add('CUP', cup_d)\n",
    "# \n",
    "# ## HebCup (First 1000)\n",
    "# hebcup_result = eval.evaluate('../result/baseline/HebCup.jsonl', 'HebCup')\n",
    "# hebcup_d = evaluate(hebcup_result)\n",
    "# rt.add('HebCup', hebcup_d)\n",
    "\n",
    "\n",
    "# LLM\n",
    "\n",
    "# result = eval.evaluate('../result/candidates/candidates-mistral-openorca-latest-9204-20240407_154640.jsonl') # openorca\n",
    "# result = eval.evaluate('../result/candidates/candidates-gemma-7b-1000-20240408_151702.jsonl') # gemma\n",
    "# result = eval.evaluate('../result/candidates/candidates-llama2-7b-1000-20240408_173014.jsonl') # llama2\n",
    "# result = eval.evaluate('../result/candidates/candidates-dolphin-mistral-latest-1000-20240408_194308.jsonl') # dolphin-mistral:latest\n",
    "# result = eval.evaluate('../result/candidates/candidates-mistral-instruct-1000-20240409_002542.jsonl') # mistral:instruct\n",
    "# result = eval.evaluate('../result/candidates/candidates-openhermes-7b-v2.5-1000-20240409_152922.jsonl') # openhermes:7b-v2.5\n",
    "# result = eval.evaluate('../result/candidates/candidates-solar-10.7b-1000-20240409_172221.jsonl') # solar:10.7b\n",
    "# result = eval.evaluate('../result/candidates/candidates-llama3-instruct-1000-20240425_161220.jsonl') # llama3\n",
    "# result = eval.evaluate('../result/candidates/candidates-llama3-instruct-9204-20240425_175633.jsonl') # llama3 all\n",
    "# result = eval.evaluate('../result/candidates/candidates-llama3-temp1.0-instruct-1000-20240427_105849.jsonl') # llama3 temp 1.0\n",
    "result = eval.evaluate('../result/candidates/candidates-llama3-temp1.5-instruct-1000-20240427_135945.jsonl') # llama3 temp 1.5\n",
    "\n",
    "\n",
    "count = len(result)\n",
    "\n",
    "simple_top1 = get_first_candidates(result, lambda x: x)\n",
    "d1 = evaluate(simple_top1)\n",
    "rt.add('LLM Top1', d1)\n",
    "\n",
    "theoretical_best = get_first_candidates(result, lambda x: sort_by_evaluation_metric(x, 'gleu', True))\n",
    "d2 = evaluate(theoretical_best)\n",
    "rt.add('LLM Optimal Top5', d2)\n",
    "\n",
    "rouge = get_first_candidates(result, lambda x: sort_by_rouge(x, 'r'))\n",
    "rt.add('LLM rouge recall', evaluate(rouge))\n",
    "\n",
    "rouge = get_first_candidates(result, lambda x: sort_by_rouge(x, 'p'))\n",
    "rt.add('LLM rouge precision', evaluate(rouge))\n",
    "\n",
    "rouge = get_first_candidates(result, lambda x: sort_by_rouge(x, 'f'))\n",
    "rt.add('LLM rouge f1', evaluate(rouge))\n",
    "\n",
    "edd = get_first_candidates(result, lambda x: sort_by_levenshtein_distance(x))\n",
    "rt.add('LLM levenshtein distance', evaluate(edd))\n",
    "\n",
    "gleu = get_first_candidates(result, lambda x: sort_by_gleu(x))\n",
    "rt.add('LLM gleu', evaluate(gleu))\n",
    "\n",
    "print(rt.to_markdown())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Method                   | Correct/total   |   Accuracy |    Ed |   Red |   Gleu |   Meteor |   Rouge-recall |   Rouge-f1 |\n",
      "|:-------------------------|:----------------|-----------:|------:|------:|-------:|---------:|---------------:|-----------:|\n",
      "| LLM Top1                 | 145/1000        |      0.145 | 8.252 | 4.59  | 47.547 |   71.578 |         75.874 |     64.951 |\n",
      "| LLM Optimal Top5         | 252/1000        |      0.252 | 4.01  | 1.841 | 63.318 |   80.981 |         83.433 |     78.039 |\n",
      "| LLM rouge recall         | 173/1000        |      0.173 | 7.685 | 4.009 | 52.375 |   76.995 |         83.148 |     70.233 |\n",
      "| LLM rouge precision      | 227/1000        |      0.227 | 4.041 | 1.832 | 60.884 |   78.544 |         81.015 |     76.783 |\n",
      "| LLM rouge f1             | 223/1000        |      0.223 | 4.299 | 1.969 | 60.476 |   79.304 |         82.249 |     76.646 |\n",
      "| LLM levenshtein distance | 229/1000        |      0.229 | 3.904 | 1.728 | 60.844 |   78.476 |         80.535 |     76.398 |\n",
      "| LLM gleu                 | 226/1000        |      0.226 | 4.198 | 1.89  | 61.172 |   79.38  |         81.957 |     76.576 |\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
